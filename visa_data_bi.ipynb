{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7865927",
   "metadata": {},
   "source": [
    "### üèóÔ∏è Pipeline de Engenharia de Dados: Survey Visa/Embratur\n",
    "\n",
    "**Objetivo:** Transformar dados brutos de pesquisa (formato *Wide*) em um Modelo Dimensional (Star Schema) otimizado para o Power BI.\n",
    "**Entrada:** `codigo_dados.xlsx`\n",
    "**Sa√≠da:** `d_respondente.csv`, `d_perguntas.csv`, `f_respostas.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1349ccc2",
   "metadata": {},
   "source": [
    "#### 1. Configura√ß√£o e Extra√ß√£o (Extract)\n",
    "\n",
    "**O que faz:** Carrega as bibliotecas, l√™ as abas do Excel e padroniza os nomes das colunas de metadados.\n",
    "**Justificativa:** O Excel de origem possui nomes de colunas inconsistentes (ex: `descricao` em vez de `texto_pergunta`). Padronizar no in√≠cio evita erros de *KeyError* nas etapas seguintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42354371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ 1. Iniciando Pipeline: Carregando e Padronizando Dados...\n",
      "   üìÇ Lendo arquivo: codigo_dados.xlsx...\n",
      "   ‚úÖ Arquivo carregado e processado com sucesso.\n",
      "   ‚úÖ Metadados e Datamap higienizados.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Dict, Optional, Any\n",
    "import re\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "FILENAME = 'codigo_dados.xlsx'\n",
    "\n",
    "def configure_environment():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "def load_excel_data(file_path: Path) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Carrega todas as abas necess√°rias do Excel de forma otimizada.\n",
    "    Retorna um dicion√°rio contendo os DataFrames.\n",
    "    \"\"\"\n",
    "    if not file_path.exists():\n",
    "        raise FileNotFoundError(f\"Arquivo '{file_path.name}' n√£o encontrado em: {file_path.parent}\")\n",
    "\n",
    "    try:\n",
    "        with pd.ExcelFile(file_path) as xls:\n",
    "            required_sheets = ['data', 'variable', 'datamap']\n",
    "            missing_sheets = [s for s in required_sheets if s not in xls.sheet_names]\n",
    "            \n",
    "            if missing_sheets:\n",
    "                raise ValueError(f\"Abas ausentes: {missing_sheets}\")\n",
    "\n",
    "            return {\n",
    "                'data': pd.read_excel(xls, sheet_name='data'),\n",
    "                'variable': pd.read_excel(xls, sheet_name='variable'),\n",
    "                'datamap': pd.read_excel(xls, sheet_name='datamap')\n",
    "            }\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Falha cr√≠tica ao ler Excel: {e}\")\n",
    "\n",
    "def process_metadata(df_variable: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Padroniza a tabela de vari√°veis (metadados).\"\"\"\n",
    "    df = df_variable.copy()\n",
    "    if 'descricao' in df.columns:\n",
    "        df = df.rename(columns={'descricao': 'texto_pergunta'})\n",
    "    \n",
    "    df.columns = ['variavel', 'posicao', 'texto_pergunta']\n",
    "    return df\n",
    "\n",
    "def process_datamap(df_datamap: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Higieniza a tabela de mapeamento (De/Para).\"\"\"\n",
    "    df = df_datamap.copy()\n",
    "    df.columns = ['variavel', 'codigo', 'label_resposta']\n",
    "    \n",
    "    for col in ['variavel', 'label_resposta']:\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "        \n",
    "    df['codigo'] = pd.to_numeric(df['codigo'], errors='coerce')\n",
    "    df = df.dropna(subset=['codigo'])\n",
    "    df['codigo'] = df['codigo'].astype(int)\n",
    "    return df\n",
    "\n",
    "def run_extraction_pipeline() -> Optional[Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]]:\n",
    "    configure_environment()\n",
    "    file_path = Path.cwd() / FILENAME\n",
    "    \n",
    "    try:\n",
    "        dfs = load_excel_data(file_path)\n",
    "        df_data = dfs['data']\n",
    "        df_variable = process_metadata(dfs['variable'])\n",
    "        df_datamap = process_datamap(dfs['datamap'])\n",
    "        return df_data, df_variable, df_datamap\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro na execu√ß√£o do Pipeline: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    resultado = run_extraction_pipeline()\n",
    "    \n",
    "    if resultado:\n",
    "        df_data, df_variable, df_datamap = resultado\n",
    "    else:\n",
    "        print(\"Falha na inicializa√ß√£o dos dados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b071ce",
   "metadata": {},
   "source": [
    "#### 2. Tratamento do Dicion√°rio (Datamap)\n",
    "\n",
    "Antes de traduzir os dados, precisamos garantir que o nosso \"tradutor\" (o Datamap) esteja limpo.\n",
    "\n",
    "**Problema:** O Excel mistura n√∫meros (`1`), textos (`\"1\"`) e floats (`1.0`), o que impede o Python de encontrar as chaves corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e2b6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ 2. Criando Dimens√£o Respondente (d_respondente)...\n",
      "   ‚úÖ Tabela criada com sucesso: 8131 linhas, 27 colunas.\n"
     ]
    }
   ],
   "source": [
    "TARGET_COLUMNS_PROFILE = (\n",
    "    'Respondent_Serial', 'GENDER_NonBinary', 'resp_age', 'QUOTAGERANGE',\n",
    "    'D1', 'D2', 'D31', 'D32', 'D33', 'D34', 'D35', 'D36',\n",
    "    'S5_1_PAIS', 'S5_1_ESTADO',\n",
    "    'S3', 'S4', 'S6', 'S7', 'TIPO', 'TEMPORADA',\n",
    "    'CurrentDay', 'CurrentMonth', 'CurrentYear',\n",
    "    'GASTO_PESSOA', 'Q24_1_VALOR', 'Q24_1_MOEDA', 'D4_1_VALOR', 'D4_1_MOEDA'\n",
    ")\n",
    "\n",
    "DATE_COLS = ['CurrentYear', 'CurrentMonth', 'CurrentDay']\n",
    "DEFAULT_YEAR = 2025\n",
    "\n",
    "def process_date_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Gera a coluna 'data_pesquisa' e 'Onda' a partir das colunas de dia/m√™s/ano.\n",
    "    Remove as colunas originais de data ap√≥s o processamento.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    if not set(DATE_COLS).issubset(df_out.columns):\n",
    "        df_out['data_pesquisa'] = pd.NaT\n",
    "        df_out['Onda'] = DEFAULT_YEAR\n",
    "        return df_out\n",
    "\n",
    "    try:\n",
    "        df_out['data_pesquisa'] = pd.to_datetime(\n",
    "            df_out['CurrentYear'].astype(str) + '-' + \n",
    "            df_out['CurrentMonth'].astype(str) + '-' + \n",
    "            df_out['CurrentDay'].astype(str),\n",
    "            errors='coerce'\n",
    "        )\n",
    "        \n",
    "        df_out['Onda'] = df_out['CurrentYear'].fillna(DEFAULT_YEAR).astype(int)\n",
    "        df_out = df_out.drop(columns=DATE_COLS, errors='ignore')\n",
    "        \n",
    "    except Exception:\n",
    "        df_out['Onda'] = DEFAULT_YEAR\n",
    "        \n",
    "    return df_out\n",
    "\n",
    "def create_respondent_dimension(df_input: pd.DataFrame, target_cols: tuple) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Orquestra a cria√ß√£o da tabela dimens√£o respondente:\n",
    "    1. Filtra colunas existentes.\n",
    "    2. Aplica tratamento de datas.\n",
    "    \"\"\"\n",
    "    available_cols = list(set(target_cols).intersection(df_input.columns))\n",
    "    \n",
    "    if 'Respondent_Serial' not in available_cols:\n",
    "        raise ValueError(\"Erro Cr√≠tico: A coluna 'Respondent_Serial' n√£o foi encontrada!\")\n",
    "\n",
    "    d_respondente = df_input[available_cols].copy()\n",
    "    return process_date_columns(d_respondente)\n",
    "\n",
    "if 'df_data' in locals():\n",
    "    d_respondente = create_respondent_dimension(df_data, TARGET_COLUMNS_PROFILE)\n",
    "else:\n",
    "    print(\"Erro: 'df_data' n√£o encontrado.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45cb6a",
   "metadata": {},
   "source": [
    "#### 3. Constru√ß√£o da Dimens√£o Respondente (`d_respondente`)\n",
    "\n",
    "Esta tabela cont√©m o **perfil √∫nico** de cada turista.\n",
    "\n",
    "**Transforma√ß√µes:**\n",
    "\n",
    "* **Data:** Unifica√ß√£o de colunas separadas (Dia/M√™s/Ano).\n",
    "* **Tradu√ß√£o Robusta:** Convers√£o de c√≥digos (`1`, `2`) para texto (`Sim`, `N√£o`) usando l√≥gica h√≠brida (Excel + Dicion√°rios Manuais para corrigir falhas na origem).\n",
    "* **Renomea√ß√£o:** Padroniza√ß√£o para *snake_case* (ex: `id_respondente`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbc7c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ 3. Executando Transforma√ß√£o e Normaliza√ß√£o...\n",
      "   üí∞ Convertendo valores para USD...\n",
      "   üè∑Ô∏è Renomeando colunas...\n",
      "   ‚úÖ Transforma√ß√£o conclu√≠da.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MAP_EMPLOYMENT = {\n",
    "    1: 'Empregado em tempo integral', 2: 'Empregado em tempo parcial', 3: 'Aut√¥nomo',\n",
    "    4: 'Desempregado (procurando)', 5: 'Desempregado (n√£o procurando)/Incapacitado', \n",
    "    6: 'Cuidador/Dono(a) de casa', 7: 'Aposentado(a)', 8: 'Estudante', 9: 'Trabalho Remoto'\n",
    "}\n",
    "\n",
    "MAP_EDUCATION = {\n",
    "    1: 'Sem estudos', 2: 'Fundamental incompleto', 3: 'Fundamental completo',\n",
    "    4: 'M√©dio incompleto', 5: 'M√©dio completo', 6: 'Superior incompleto',\n",
    "    7: 'Superior completo', 8: 'P√≥s-Gradua√ß√£o', 9: 'Mestrado', 10: 'Doutorado'\n",
    "}\n",
    "\n",
    "MAP_COMPANIONS = {\n",
    "    1: 'Apenas eu', 2: 'Companheiro(a)', 3: 'Filho(s)',\n",
    "    4: 'Pais', 5: 'Parentes', 6: 'Amigos'\n",
    "}\n",
    "\n",
    "MAP_CURRENCY_NAMES = {\n",
    "    1: 'ARS', 2: 'USD', 3: 'CLP', 4: 'PYG', 5: 'UYU', 6: 'EUR', \n",
    "    7: 'GBP', 8: 'CAD', 9: 'COP', 10: 'PEN', 11: 'MXN', 12: 'BRL'\n",
    "}\n",
    "\n",
    "EXCHANGE_RATES = {\n",
    "    'ARS': 0.00069, 'USD': 1.0, 'CLP': 0.0011, 'PYG': 0.00015,\n",
    "    'UYU': 0.0256, 'EUR': 1.18, 'GBP': 1.35, 'CAD': 0.73,\n",
    "    'COP': 0.00026, 'PEN': 0.29, 'MXN': 0.056, 'BRL': 0.18\n",
    "}\n",
    "\n",
    "RENAME_SCHEMA = {\n",
    "    'Respondent_Serial': 'id_respondente', 'GENDER_NonBinary': 'genero', \n",
    "    'resp_age': 'idade', 'QUOTAGERANGE': 'faixa_etaria', \n",
    "    'S5_1_PAIS': 'pais', 'S5_1_ESTADO': 'estado_residencia', \n",
    "    'S3': 'reside_brasil', 'S4': 'viagem_int_12m', \n",
    "    'S6': 'meio_transporte', 'S7': 'portao_entrada',\n",
    "    'TIPO': 'tipo_turista', 'TEMPORADA': 'temporada', \n",
    "    'GASTO_PESSOA': 'gasto_total_calculado',\n",
    "    'D1': 'status_emprego', 'D2': 'escolaridade',\n",
    "    'Q24_1_VALOR': 'gasto_viagem_original', 'Q24_1_MOEDA': 'moeda_viagem',\n",
    "    'D4_1_VALOR': 'renda_familiar_original', 'D4_1_MOEDA': 'moeda_renda'\n",
    "}\n",
    "\n",
    "def get_mapping_for_column(col_name: str, df_map: pd.DataFrame) -> Dict[int, str]:\n",
    "    subset = df_map[df_map['variavel'] == col_name]\n",
    "    return {} if subset.empty else dict(zip(subset['codigo'], subset['label_resposta']))\n",
    "\n",
    "def safe_translate(value: Any, mapping: Dict[Any, str]) -> str:\n",
    "    if pd.isna(value) or str(value).strip() == '':\n",
    "        return value\n",
    "        \n",
    "    val_str = str(value)\n",
    "    \n",
    "    if ';' not in val_str:\n",
    "        try:\n",
    "            code = int(float(val_str.split('.')[0]))\n",
    "            return str(mapping.get(code, val_str))\n",
    "        except ValueError:\n",
    "            return val_str\n",
    "\n",
    "    translated_parts = []\n",
    "    for p in val_str.split(';'):\n",
    "        try:\n",
    "            code = int(float(p.strip()))\n",
    "            translated_parts.append(str(mapping.get(code, p)))\n",
    "        except ValueError:\n",
    "            translated_parts.append(p)\n",
    "            \n",
    "    return \"; \".join(translated_parts)\n",
    "\n",
    "def calculate_usd_vectorized(df: pd.DataFrame, val_col: str, currency_col: str, result_col: str) -> pd.DataFrame:\n",
    "    if val_col not in df.columns or currency_col not in df.columns:\n",
    "        return df\n",
    "\n",
    "    currency_codes = df[currency_col].map(MAP_CURRENCY_NAMES).fillna(df[currency_col])\n",
    "    currency_codes = currency_codes.astype(str).str.upper()\n",
    "    \n",
    "    rates = currency_codes.map(EXCHANGE_RATES)\n",
    "    mask_nan = rates.isna()\n",
    "    \n",
    "    if mask_nan.any():\n",
    "        def infer_rate(text):\n",
    "            if 'REAL' in text: return 0.18\n",
    "            if 'EURO' in text: return 1.18\n",
    "            if 'PESO ARG' in text: return 0.00069\n",
    "            return 1.0 \n",
    "        rates[mask_nan] = currency_codes[mask_nan].apply(infer_rate)\n",
    "\n",
    "    df[result_col] = (df[val_col] * rates).round(2)\n",
    "    return df\n",
    "\n",
    "def run_transformation_pipeline(df: pd.DataFrame, df_map: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_out = df.copy()\n",
    "\n",
    "    cols_to_translate = [\n",
    "        'GENDER_NonBinary', 'TEMPORADA', 'TIPO', 'S5_1_PAIS', 'S3', 'S4', \n",
    "        'S6', 'S7', 'QUOTAGERANGE', 'D1', 'D2', \n",
    "        'D31', 'D32', 'D33', 'D34', 'D35', 'D36', \n",
    "        'Q24_1_MOEDA', 'D4_1_MOEDA'\n",
    "    ]\n",
    "\n",
    "    for col in cols_to_translate:\n",
    "        if col not in df_out.columns:\n",
    "            continue\n",
    "            \n",
    "        mapping = {}\n",
    "        if col == 'D1': mapping = MAP_EMPLOYMENT\n",
    "        elif col == 'D2': mapping = MAP_EDUCATION\n",
    "        elif col.startswith('D3'): mapping = MAP_COMPANIONS\n",
    "        elif 'MOEDA' in col: mapping = MAP_CURRENCY_NAMES\n",
    "        else:\n",
    "            mapping = get_mapping_for_column(col, df_map)\n",
    "            \n",
    "        if mapping:\n",
    "            df_out[col] = df_out[col].apply(lambda x: safe_translate(x, mapping))\n",
    "\n",
    "    df_out = calculate_usd_vectorized(df_out, 'Q24_1_VALOR', 'Q24_1_MOEDA', 'gasto_viagem_usd')\n",
    "    df_out = calculate_usd_vectorized(df_out, 'D4_1_VALOR', 'D4_1_MOEDA', 'renda_familiar_usd')\n",
    "\n",
    "    df_out = df_out.rename(columns=RENAME_SCHEMA)\n",
    "    \n",
    "    if 'estado_residencia' in df_out.columns:\n",
    "        df_out['estado_residencia'] = (\n",
    "            df_out['estado_residencia']\n",
    "            .astype(str).str.title().str.strip()\n",
    "            .replace({'Nan': 'N√£o Informado', 'Null': 'N√£o Informado'})\n",
    "        )\n",
    "\n",
    "    return df_out\n",
    "\n",
    "if 'd_respondente' in locals() and 'df_datamap' in locals():\n",
    "    d_respondente = run_transformation_pipeline(d_respondente, df_datamap)\n",
    "else:\n",
    "    print(\"Erro: Depend√™ncias n√£o encontradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee234821",
   "metadata": {},
   "source": [
    "#### 4. Constru√ß√£o da Fato Respostas (`f_respostas`)\n",
    "\n",
    "Esta √© a tabela longa (Unpivoted) que cont√©m todas as respostas. \n",
    "\n",
    "**Otimiza√ß√µes:**\n",
    "\n",
    "* **Melt:** Transforma colunas em linhas.\n",
    "* **Limpeza de Nulos:** Remove linhas vazias para reduzir drasticamente o tamanho do arquivo.\n",
    "* **Fallback de Texto:** Se n√£o houver tradu√ß√£o (ex: \"Outro pa√≠s: Chile\"), mant√©m o texto original digitado pelo usu√°rio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f76a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ 4. Criando Fato Respostas (f_respostas)...\n",
      "   üî• Unpivoting (Melt) dos dados...\n",
      "   üìñ Traduzindo c√≥digos via Datamap...\n",
      "   üîß Aplicando patches (Acomoda√ß√£o, Gastronomia, Atividades)...\n",
      "   üåç Higienizando textos e gerando dados geogr√°ficos...\n",
      "   ‚úÖ f_respostas criada com 1443985 linhas.\n"
     ]
    }
   ],
   "source": [
    "IGNORE_COLS = [\n",
    "    'Respondent_Serial', 'CurrentDay', 'CurrentMonth', 'CurrentYear', \n",
    "    'GENDER_NonBinary', 'resp_age', 'QUOTAGERANGE', 'S5_1_PAIS', 'S5_1_ESTADO', 'TIPO',\n",
    "    'Q24_1_VALOR', 'Q24_1_MOEDA', 'D4_1_VALOR', 'D4_1_MOEDA' \n",
    "]\n",
    "\n",
    "FIX_ACCOMMODATION = {\n",
    "    1: 'Hotel de 3 estrelas ou menos', 2: 'Hotel de 4 ou 5 estrelas', 3: 'Pousada',\n",
    "    4: 'Resort', 5: 'Casa de amigos e parentes', 6: 'Im√≥vel pr√≥prio',\n",
    "    7: 'Im√≥vel alugado por temporada', 8: 'Airbnb e similares', 9: 'Albergue/hostel',\n",
    "    10: 'Camping', 11: 'Cruzeiro / Navio', 98: 'Outros'\n",
    "}\n",
    "\n",
    "FIX_GASTRONOMY = {\n",
    "    1: 'Restaurante de alta gastronomia', 2: 'Restaurante por quilo/buffet', 3: 'Fast Food',\n",
    "    4: 'Cafeterias', 5: '√âtnicos', 6: 'Bistr√¥',\n",
    "    7: 'Food truck', 8: 'Restaurantes √† la carte', 98: 'Outros'\n",
    "}\n",
    "\n",
    "FIX_ACTIVITIES = {\n",
    "    1: 'Tratamentos m√©dicos/est√©ticos', 2: 'Terapias de bem-estar',\n",
    "    4: 'Turismo m√≠stico/esot√©rico', 5: 'Atividades n√°uticas',\n",
    "    6: 'Cruzeiros', 7: 'Visitas culturais',\n",
    "    8: 'Comunidades tradicionais', 9: 'Eventos culturais',\n",
    "    10: 'Gastronomia', 11: 'Esportes (geral)',\n",
    "    12: 'Eventos esportivos', 13: 'Mergulho',\n",
    "    14: 'Aventura (trilhas, rafting)', 15: 'Observa√ß√£o fauna/flora',\n",
    "    16: 'Turismo rural', 17: 'Parques tem√°ticos',\n",
    "    18: 'Ecoturismo', 19: 'Compras',\n",
    "    20: 'Visitar amigos/parentes', 21: 'Vida Noturna',\n",
    "    22: 'Sol e praia', 23: 'Carnaval de rua',\n",
    "    24: 'Carnaval samb√≥dromo', 98: 'Outros', 99: 'N√£o realizou atividades'\n",
    "}\n",
    "\n",
    "MAP_UF_NAME = {\n",
    "    'AC': 'Acre', 'AL': 'Alagoas', 'AP': 'Amapa', 'AM': 'Amazonas', 'BA': 'Bahia', 'CE': 'Ceara',\n",
    "    'DF': 'Distrito Federal', 'ES': 'Espirito Santo', 'GO': 'Goias', 'MA': 'Maranhao', 'MT': 'Mato Grosso',\n",
    "    'MS': 'Mato Grosso do Sul', 'MG': 'Minas Gerais', 'PA': 'Para', 'PB': 'Paraiba', 'PR': 'Parana',\n",
    "    'PE': 'Pernambuco', 'PI': 'Piaui', 'RJ': 'Rio de Janeiro', 'RN': 'Rio Grande do Norte',\n",
    "    'RS': 'Rio Grande do Sul', 'RO': 'Rondonia', 'RR': 'Roraima', 'SC': 'Santa Catarina',\n",
    "    'SP': 'Sao Paulo', 'SE': 'Sergipe', 'TO': 'Tocantins'\n",
    "}\n",
    "\n",
    "def unpivot_data(df: pd.DataFrame, ignore_list: list) -> pd.DataFrame:\n",
    "    \"\"\"Transforma colunas em linhas (Melt) e remove nulos.\"\"\"\n",
    "    cols_to_melt = [c for c in df.columns if c not in ignore_list]\n",
    "    \n",
    "    df_melt = df.melt(\n",
    "        id_vars=['Respondent_Serial'], \n",
    "        value_vars=cols_to_melt, \n",
    "        var_name='cod_pergunta', \n",
    "        value_name='cod_resposta'\n",
    "    )\n",
    "    \n",
    "    df_melt = df_melt.dropna(subset=['cod_resposta'])\n",
    "    df_melt = df_melt[df_melt['cod_resposta'].astype(str).str.strip() != '']\n",
    "    return df_melt\n",
    "\n",
    "def apply_translations(df_fact: pd.DataFrame, df_map: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Realiza o Left Join com o Datamap para trazer os textos das respostas.\"\"\"\n",
    "    merged = df_fact.merge(\n",
    "        df_map, \n",
    "        left_on=['cod_pergunta', 'cod_resposta'], \n",
    "        right_on=['variavel', 'codigo'], \n",
    "        how='left'\n",
    "    )\n",
    "    merged['resposta_texto'] = merged['label_resposta'].fillna(merged['cod_resposta'])\n",
    "    return merged\n",
    "\n",
    "def patch_specific_questions(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Aplica corre√ß√µes manuais (Dicion√°rios) baseadas em Regex de perguntas.\"\"\"\n",
    "    df_out = df.copy()\n",
    "\n",
    "    def _translate_item(val, mapping):\n",
    "        val_str = str(val).strip()\n",
    "        try:\n",
    "            return mapping.get(int(float(val_str)), val_str)\n",
    "        except:\n",
    "            if ';' in val_str:\n",
    "                parts = [str(mapping.get(int(float(p)), p)) if p.replace('.','',1).isdigit() else p for p in val_str.split(';')]\n",
    "                return \"; \".join(parts)\n",
    "            return val_str\n",
    "\n",
    "    patches = [\n",
    "        (r'^Q15|^Q16', FIX_ACCOMMODATION),\n",
    "        (r'^Q18', FIX_GASTRONOMY),\n",
    "        (r'^Q23', FIX_ACTIVITIES)\n",
    "    ]\n",
    "\n",
    "    for pattern, mapping in patches:\n",
    "        mask = df_out['cod_pergunta'].astype(str).str.contains(pattern, regex=True, na=False)\n",
    "        if mask.any():\n",
    "            df_out.loc[mask, 'resposta_texto'] = df_out.loc[mask, 'cod_resposta'].apply(lambda x: _translate_item(x, mapping))\n",
    "\n",
    "    return df_out\n",
    "\n",
    "def clean_and_enrich(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Limpeza final de textos (Outros) e cria√ß√£o de colunas geogr√°ficas.\"\"\"\n",
    "    df_out = df.copy()\n",
    "    \n",
    "    df_out['resposta_texto'] = df_out['resposta_texto'].astype(str).str.replace(r'^Outros.*', 'Outros', regex=True)\n",
    "    df_out['UF_Mapa'] = df_out['resposta_texto'].str.extract(r'\\((.*?)\\)')\n",
    "    df_out['Nome_Estado_Mapa'] = df_out['UF_Mapa'].map(MAP_UF_NAME)\n",
    "    \n",
    "    df_out = df_out.rename(columns={'Respondent_Serial': 'id_respondente'})\n",
    "    cols_final = ['id_respondente', 'cod_pergunta', 'cod_resposta', 'resposta_texto', 'Nome_Estado_Mapa']\n",
    "    return df_out[cols_final]\n",
    "\n",
    "def create_fact_responses(df_raw_input: pd.DataFrame, df_map_input: pd.DataFrame) -> pd.DataFrame:\n",
    "    f_respostas = unpivot_data(df_raw_input, IGNORE_COLS)\n",
    "    f_respostas = apply_translations(f_respostas, df_map_input)\n",
    "    f_respostas = patch_specific_questions(f_respostas)\n",
    "    f_respostas = clean_and_enrich(f_respostas)\n",
    "    return f_respostas\n",
    "\n",
    "if 'df_data' in locals() and 'df_datamap' in locals():\n",
    "    f_respostas = create_fact_responses(df_data, df_datamap)\n",
    "else:\n",
    "    print(\"Erro: Depend√™ncias n√£o encontradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695d1161",
   "metadata": {},
   "source": [
    "#### 5. Constru√ß√£o da Dimens√£o Perguntas (`d_perguntas`)\n",
    "\n",
    "Tabela de metadados para criar menus e filtros no Dashboard.\n",
    "\n",
    "**Transforma√ß√µes:**\n",
    "\n",
    "* **Categoriza√ß√£o:** Cria√ß√£o da coluna `categoria` baseada nos prefixos (Q1, Q24, S...) para permitir navega√ß√£o por menu.\n",
    "* **Limpeza:** Remo√ß√£o de tags t√©cnicas (`[HIDDEN]`) e caracteres indesejados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbb6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ 5. Criando Dimens√£o Perguntas (d_perguntas)...\n",
      "   üóÇÔ∏è Categorizando perguntas para menu de navega√ß√£o...\n",
      "   üßπ Removendo perguntas t√©cnicas e sem dados...\n",
      "   ‚úÖ d_perguntas criada com 409 perguntas categorizadas.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "CATEGORY_RULES = [\n",
    "    ('1. Perfil do Turista',        ['D', 'GENDER', 'RESP_AGE', 'QUOTA', 'S5_', 'Q1', 'Q2']),\n",
    "    ('2. Planejamento e Marketing', ['Q3', 'Q4', 'Q5', 'Q6', 'Q23A']),\n",
    "    ('3. Caracter√≠sticas da Viagem',['S', 'Q7', 'Q8', 'Q9', 'Q10', 'Q14', 'Q19', 'Q20', 'Q21', 'Q22']),\n",
    "    ('4. Destinos Visitados',       ['Q11', 'Q12', 'Q13']),\n",
    "    ('5. Hospedagem e Transporte',  ['Q15', 'Q16', 'Q17']),\n",
    "    ('6. Atividades e Gastronomia', ['Q18', 'Q23']),\n",
    "    ('7. Gastos e Pagamentos',      ['Q24', 'Q25', 'Q26', 'Q27', 'Q28', 'Q29', 'Q30', 'Q31', 'Q32', 'GASTO']),\n",
    "    ('8. Avalia√ß√£o e Imagem',       ['Q33', 'Q34', 'Q35', 'Q36', 'Q37', 'Q38', 'Q39'])\n",
    "]\n",
    "\n",
    "TECHNICAL_JUNK = [\n",
    "    'TIPO', 'RESPONDENT_SERIAL', 'CURRENTDAY', 'CURRENTMONTH', 'CURRENTYEAR'\n",
    "]\n",
    "\n",
    "def get_category(code: str) -> str:\n",
    "    \"\"\"Define a categoria baseada no c√≥digo da pergunta.\"\"\"\n",
    "    code_upper = str(code).upper().strip()\n",
    "    \n",
    "    for category, prefixes in CATEGORY_RULES:\n",
    "        if any(code_upper.startswith(p) for p in prefixes):\n",
    "            if code_upper.startswith('D') and not (len(code_upper) > 1 and code_upper[1].isdigit()):\n",
    "                continue \n",
    "            return category\n",
    "            \n",
    "    return '9. Outros / T√©cnico'\n",
    "\n",
    "def clean_question_text(text_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Limpeza vetorizada de textos das perguntas (Regex).\"\"\"\n",
    "    clean = text_series.astype(str).str.strip()\n",
    "    clean = clean.str.replace(r'\\s*:\\s*$', '', regex=True)\n",
    "    clean = clean.str.replace(r'\\[.*?\\]', '', regex=True)\n",
    "    clean = clean.str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "    return clean\n",
    "\n",
    "def create_dim_questions(df_vars: pd.DataFrame, df_fact: pd.DataFrame) -> pd.DataFrame:\n",
    "    df_dim = df_vars[['variavel', 'texto_pergunta']].copy()\n",
    "    df_dim.columns = ['cod_pergunta', 'texto_pergunta']\n",
    "    \n",
    "    df_dim['categoria'] = df_dim['cod_pergunta'].apply(get_category)\n",
    "    \n",
    "    valid_questions = set(df_fact['cod_pergunta'].unique())\n",
    "    \n",
    "    is_junk = df_dim['cod_pergunta'].str.upper().isin(TECHNICAL_JUNK)\n",
    "    is_coded = df_dim['cod_pergunta'].str.contains('_Coded_|_SEM_OUTLIERS', na=False)\n",
    "    is_orphan = ~df_dim['cod_pergunta'].isin(valid_questions)\n",
    "    \n",
    "    df_dim = df_dim[~(is_junk | is_coded | is_orphan)].copy()\n",
    "    \n",
    "    df_dim['texto_pergunta'] = clean_question_text(df_dim['texto_pergunta'])\n",
    "    \n",
    "    return df_dim.sort_values(by=['categoria', 'cod_pergunta'])\n",
    "\n",
    "if 'df_variable' in locals() and 'f_respostas' in locals():\n",
    "    d_perguntas = create_dim_questions(df_variable, f_respostas)\n",
    "else:\n",
    "    print(\"Erro: Depend√™ncias n√£o encontradas.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb61c3a",
   "metadata": {},
   "source": [
    "#### 6. Exporta√ß√£o (Load)\n",
    "\n",
    "Salva os Dataframes em CSV prontos para importa√ß√£o no Power BI. Usamos `;` como separador para evitar conflitos com v√≠rgulas nos textos das perguntas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78f8b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ 6. Iniciando Higieniza√ß√£o Final e Exporta√ß√£o...\n",
      "   üßπ Limpando tabela: d_respondente...\n",
      "   üßπ Limpando tabela: d_perguntas...\n",
      "   üßπ Limpando tabela: f_respostas...\n",
      "   üíæ Salvando arquivos CSV...\n",
      "--------------------------------------------------\n",
      "üöÄ SUCESSO TOTAL! Pipeline conclu√≠do.\n",
      "   Arquivos gerados na pasta:\n",
      "   1. d_perguntas.csv\n",
      "   2. d_respondente.csv\n",
      "   3. f_respostas.csv\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "EXPORT_CONFIG = {\n",
    "    'sep': ';',\n",
    "    'encoding': 'utf-8-sig',\n",
    "    'index': False,\n",
    "    'date_format': '%Y-%m-%d'\n",
    "}\n",
    "\n",
    "BASE_PATH = Path.cwd()\n",
    "\n",
    "def clean_text_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Higieniza colunas de texto removendo quebras de linha e espa√ßos extras.\n",
    "    Trata valores nulos antes da convers√£o para string para evitar 'nan' literais.\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    text_cols = df_out.select_dtypes(include=['object', 'string']).columns\n",
    "    \n",
    "    if len(text_cols) > 0:\n",
    "        # Preenche nulos com vazio antes de converter para string\n",
    "        df_out[text_cols] = df_out[text_cols].fillna('')\n",
    "        \n",
    "        for col in text_cols:\n",
    "            df_out[col] = (\n",
    "                df_out[col].astype(str)\n",
    "                .str.replace(r'[\\r\\n]+', ' ', regex=True)\n",
    "                .str.replace(r'\\s+', ' ', regex=True)\n",
    "                .str.strip()\n",
    "            )\n",
    "            \n",
    "    return df_out\n",
    "\n",
    "def export_csv(df: pd.DataFrame, filename: str):\n",
    "    \"\"\"Aplica limpeza final e salva o arquivo com as configura√ß√µes padr√£o.\"\"\"\n",
    "    try:\n",
    "        df_clean = clean_text_columns(df)\n",
    "        file_path = BASE_PATH / filename\n",
    "        df_clean.to_csv(file_path, **EXPORT_CONFIG)\n",
    "    except PermissionError:\n",
    "        print(f\"Erro de Permiss√£o: O arquivo '{filename}' est√° aberto no Excel/Power BI.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erro cr√≠tico ao salvar '{filename}': {e}\")\n",
    "\n",
    "def run_load_pipeline():\n",
    "    artifacts = {\n",
    "        'd_perguntas.csv': d_perguntas if 'd_perguntas' in locals() else None,\n",
    "        'd_respondente.csv': d_respondente if 'd_respondente' in locals() else None,\n",
    "        'f_respostas.csv': f_respostas if 'f_respostas' in locals() else None\n",
    "    }\n",
    "    \n",
    "    for filename, df_obj in artifacts.items():\n",
    "        if df_obj is not None:\n",
    "            export_csv(df_obj, filename)\n",
    "        else:\n",
    "            print(f\"Aviso: DataFrame para '{filename}' n√£o encontrado na mem√≥ria.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_load_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
